---
title: "Navigation Task Model Comparison Analysis"
author: "Yasmine Bassil"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 10, fig.height = 6)
```

# Overview

This analysis compares different statistical models for the NavCity navigation task, examining age differences between Young Adults (YAs) and Older Adults (OAs) across multiple behavioral outcomes.

**Research Questions:**
- How do YAs and OAs differ in navigation performance across learning blocks?
- How do YAs and OAs differ in navigation performance across different targets?
- Should Target be treated as a fixed or random effect?

# Setup and Data Loading

```{r load-libraries}
# Load required libraries
library(lme4)
library(dplyr)
library(broom.mixed)
library(knitr)
library(ggplot2)
library(gridExtra)
library(kableExtra)  # Added this for table styling
```

```{r define-functions}
# Define the load_and_standardize function
load_and_standardize <- function(file_path, age_group) {
  # Load the data
  data <- read.csv(file_path)
  
  # Rename first column to "Index" for consistency across datasets
  names(data)[1] <- "Index"
  
  # Add age group column
  data$Age <- age_group
  
  # Ensure proper data types
  data$Age <- as.factor(data$Age)
  data$Block_Num <- as.factor(data$Block_Num)
  data$Target_Name <- as.factor(data$Target_Name)
  data$Participant <- as.factor(data$Participant)
  
  # Add any other standardization steps here
  # For example:
  # - Remove outliers
  # - Handle missing values
  # - Create derived variables
  
  return(data)
}

# Set data path (modify this to your actual data directory)
data_path <- "/Volumes/YB_Drive/NavAging_Paper/data"  # UPDATE THIS PATH
```

```{r load-data}
# Load and combine data
ya_b_t_data <- load_and_standardize(file.path(data_path, 'ya_merged_results.csv'), "YA")
oa_b_t_data <- load_and_standardize(file.path(data_path, 'oa_merged_results.csv'), "OA")

# Combine YA and OA data
nav_data <- rbind(ya_b_t_data, oa_b_t_data)

# Check data structure
cat("Data loaded successfully!\n")
cat("Total observations:", nrow(nav_data), "\n")
cat("YA observations:", sum(nav_data$Age == "YA"), "\n")
cat("OA observations:", sum(nav_data$Age == "OA"), "\n")
cat("Unique participants:", length(unique(nav_data$Participant)), "\n")
cat("Blocks:", sort(unique(nav_data$Block_Num)), "\n")
cat("Targets:", sort(unique(nav_data$Target_Name)), "\n")
```

```{r data-summary}
# Summary statistics by age group
nav_data %>%
  group_by(Age) %>%
  summarise(
    N_participants = n_distinct(Participant),
    N_observations = n(),
    .groups = 'drop'
  ) %>%
  kable(caption = "Sample Size by Age Group")
```

# Model Fitting and Comparison

## Define Outcomes and Run Models

```{r define-outcomes}
# Define outcome variables
outcomes <- c("Total_Time", "Orientation_Time", "Navigation_Time", "Speed", 
              "Distance", "Mean_Dwell", "Teleportations", 
              "Mean_Teleport_Distance")

# Initialize lists to store results
model_results <- list()
model_comparison <- data.frame()
convergence_issues <- list()
```

```{r fit-models}
# Loop through each outcome
for(outcome in outcomes) {
  
  cat("Processing outcome:", outcome, "\n")
  
  # Create dynamic formula strings
  formula_block <- paste(outcome, "~ Age * Block_Num + (1 | Participant)")
  formula_target <- paste(outcome, "~ Age * Target_Name + (1 | Participant)")
  formula_target_random <- paste(outcome, "~ Age + (1 | Participant) + (1 | Target_Name)")
  
  # Combined model formulas (MODELS 4-6)
  formula_combined_full <- paste(outcome, "~ Age * Block_Num * Target_Name + (1 | Participant)")
  formula_combined_reduced <- paste(outcome, "~ Age * Block_Num + Age * Target_Name + Block_Num * Target_Name + (1|Participant)")
  formula_combined_simple <- paste(outcome, "~ Age + Block_Num + Target_Name + Age:Block_Num + Age:Target_Name + (1 | Participant)")
  
  # Try to fit models with error handling
  tryCatch({
    
    # Original models (MODELS 1-3)
    cat("  Fitting original models...\n")
    model_block <- lmer(as.formula(formula_block), data = nav_data)
    cat("    Block model: SUCCESS\n")
    
    model_target_fixed <- lmer(as.formula(formula_target), data = nav_data)
    cat("    Target fixed model: SUCCESS\n")
    
    model_target_random <- lmer(as.formula(formula_target_random), data = nav_data)
    cat("    Target random model: SUCCESS\n")
    
    # Combined models (MODELS 4-6)
    cat("  Attempting combined models...\n")
    
    # Model 4: Full three-way interaction
    model_combined_full <- tryCatch({
      cat("    Fitting full combined model...\n")
      result <- lmer(as.formula(formula_combined_full), data = nav_data)
      cat("    Full combined model: SUCCESS\n")
      result
    }, error = function(e) {
      cat("    Full combined model failed:", e$message, "\n")
      NULL
    })
    
    # Model 5: All two-way interactions, no three-way
    model_combined_reduced <- tryCatch({
      cat("    Fitting reduced combined model...\n")
      result <- lmer(as.formula(formula_combined_reduced), data = nav_data)
      cat("    Reduced combined model: SUCCESS\n")
      result
    }, error = function(e) {
      cat("    Reduced combined model failed:", e$message, "\n")
      NULL
    })
    
    # Model 6: Simple additive + key interactions
    model_combined_simple <- tryCatch({
      cat("    Fitting simple combined model...\n")
      result <- lmer(as.formula(formula_combined_simple), data = nav_data)
      cat("    Simple combined model: SUCCESS\n")
      result
    }, error = function(e) {
      cat("    Simple combined model failed:", e$message, "\n")
      NULL
    })
    
    # Debug: Check what models actually exist
    cat("  Models created: Block=", !is.null(model_block), 
        " | Target_Fixed=", !is.null(model_target_fixed),
        " | Target_Random=", !is.null(model_target_random),
        " | Combined_Full=", !is.null(model_combined_full),
        " | Combined_Reduced=", !is.null(model_combined_reduced), 
        " | Combined_Simple=", !is.null(model_combined_simple), "\n")
    
    # Store models
    model_results[[outcome]] <- list(
      block_model = model_block,
      target_fixed = model_target_fixed,
      target_random = model_target_random,
      combined_full = model_combined_full,
      combined_reduced = model_combined_reduced,
      combined_simple = model_combined_simple
    )
    
    # Check convergence
    block_converged <- is.null(model_block@optinfo$conv$lme4$messages)
    target_fixed_converged <- is.null(model_target_fixed@optinfo$conv$lme4$messages)
    target_random_converged <- is.null(model_target_random@optinfo$conv$lme4$messages)
    combined_full_converged <- is.null(model_combined_full@optinfo$conv$lme4$messages)
    combined_reduced_converged <- is.null(model_combined_reduced@optinfo$conv$lme4$messages)
    combined_simple_converged <- is.null(model_combined_simple@optinfo$conv$lme4$messages)
    
    # Store convergence info
    convergence_issues[[outcome]] <- list(
      block_converged = block_converged,
      target_fixed_converged = target_fixed_converged,
      target_random_converged = target_random_converged,
      combined_full_converged = combined_full_converged,
      combined_reduced_converged = combined_reduced_converged,
      combined_simple_converged = combined_simple_converged
    )
    
    # Extract model comparison metrics
    block_aic <- AIC(model_block)
    block_bic <- BIC(model_block)
    
    target_fixed_aic <- AIC(model_target_fixed)
    target_fixed_bic <- BIC(model_target_fixed)
    
    target_random_aic <- AIC(model_target_random)
    target_random_bic <- BIC(model_target_random)
    
    combined_full_aic <- AIC(model_combined_full)
    combined_full_bic <- BIC(model_combined_full)
    
    combined_reduced_aic <- AIC(model_combined_reduced)
    combined_reduced_bic <- BIC(model_combined_reduced)
    
    combined_simple_aic <- AIC(model_combined_simple)
    combined_simple_bic <- BIC(model_combined_simple)
    
    # Add to comparison dataframe
    temp_comparison <- data.frame(
      Outcome = outcome,
      Model = c("Age_x_Block", "Age_x_Target_Fixed", "Age_Target_Random", 
                "Full_Model", "Full_Model_Reduced", "Full_Model_Simple"),
      AIC = c(block_aic, target_fixed_aic, target_random_aic,
              combined_full_aic, combined_reduced_aic, combined_simple_aic),
      BIC = c(block_bic, target_fixed_bic, target_random_bic,
              combined_full_bic, combined_reduced_bic, combined_simple_bic),
      N_params = c(length(fixef(model_block)) + 1, 
                   length(fixef(model_target_fixed)) + 1,
                   length(fixef(model_target_random)) + 2,
                   length(fixef(model_combined_full)) + 2,
                   length(fixef(model_combined_reduced)) + 2,
                   length(fixef(model_combined_simple)) + 3),
      Converged = c(block_converged, target_fixed_converged, target_random_converged,
                    combined_full_converged, combined_reduced_converged, combined_simple_converged)
    )
    
    model_comparison <- rbind(model_comparison, temp_comparison)
    
    # Print quick summary for this outcome
    cat("  AIC - Block:", round(block_aic, 1), 
        "| Target Fixed:", round(target_fixed_aic, 1),
        "| Target Random:", round(target_random_aic, 1),
        "| Combined Full:", round(combined_full_aic, 1),
        "| Combined Reduced:", round(combined_reduced_aic, 1),
        "| Combined Simple:", round(combined_simple_aic, 1),
        "\n")
    
  }, error = function(e) {
    cat("  ERROR fitting models for", outcome, ":", e$message, "\n")
    
    # Store error info
    convergence_issues[[outcome]] <- list(
      block_converged = FALSE,
      target_fixed_converged = FALSE,
      target_random_converged = FALSE,
      combined_full_converged = FALSE,
      combined_reduced_converged = FALSE,
      combined_simple_converged = FALSE,
      error = e$message
    )
  })
}
```

## Overfitting Assessment

```{r overfitting-assessment}
# Create overfitting assessment
overfitting_summary <- data.frame()

for(outcome in names(model_results)) {
  models <- model_results[[outcome]]
  conv_info <- convergence_issues[[outcome]]
  
  # Check if models exist and converged
  block_ok <- !is.null(models$block_model) && conv_info$block_converged
  target_fixed_ok <- !is.null(models$target_fixed) && conv_info$target_fixed_converged
  target_random_ok <- !is.null(models$target_random) && conv_info$target_random_converged
  combined_full_ok <- !is.null(models$combined_full) && conv_info$combined_full_converged
  combined_reduced_ok <- !is.null(models$combined_reduced) && conv_info$combined_reduced_converged
  combined_simple_ok <- !is.null(models$combined_simple) && conv_info$combined_simple_converged
  
  # Additional checks for overfitting indicators
  if(block_ok) {
    # Check for boundary estimates in random effects
    block_var_components <- as.data.frame(VarCorr(models$block_model))
    block_boundary <- any(block_var_components$vcov < 0.001, na.rm = TRUE)
    block_ok <- block_ok && !block_boundary
  }
  
  if(target_random_ok) {
    # Check for boundary estimates in random effects  
    target_var_components <- as.data.frame(VarCorr(models$target_random))
    target_boundary <- any(target_var_components$vcov < 0.001, na.rm = TRUE)
    target_random_ok <- target_random_ok && !target_boundary
  }
  
  # Add to summary
  temp_summary <- data.frame(
    Outcome = outcome,
    Block_Model_OK = block_ok,
    Target_Fixed_OK = target_fixed_ok,
    Target_Random_OK = target_random_ok,
    Block_Converged = conv_info$block_converged,
    Target_Fixed_Converged = conv_info$target_fixed_converged,
    Target_Random_Converged = conv_info$target_random_converged,
    Combined_Model_OK = combined_full_ok,
    Combined_Reduced_OK = combined_reduced_ok,
    Combined_Simple_OK = combined_simple_ok,
    Combined_Full_Converged = conv_info$combined_full_converged,
    Combined_Reduced_Converged = conv_info$combined_reduced_converged,
    Combined_Simple_Converged = conv_info$combined_simple_converged
  )
  
  overfitting_summary <- rbind(overfitting_summary, temp_summary)
}
```

## Model Comparison Results

```{r process-comparison}
# Calculate AIC/BIC differences for easier interpretation
model_comparison <- model_comparison %>%
  group_by(Outcome) %>%
  mutate(
    Delta_AIC = AIC - min(AIC),
    Delta_BIC = BIC - min(BIC),
    Best_AIC = ifelse(Delta_AIC == 0, "***", 
                     ifelse(Delta_AIC < 2, "*", "")),
    Best_BIC = ifelse(Delta_BIC == 0, "***", 
                     ifelse(Delta_BIC < 2, "*", ""))
  ) %>%
  ungroup()
```

```{r display-comparison-table}
# Display results table
model_comparison %>%
  select(Outcome, Model, AIC, Delta_AIC, Best_AIC, BIC, Delta_BIC, Best_BIC, Converged) %>%
  kable(digits = 1, 
        caption = "Model Comparison Results",
        col.names = c("Outcome", "Model", "AIC", "ΔAIC", "Best", "BIC", "ΔBIC", "Best", "Conv.")) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

```{r interpretation-guide}
cat("=== INTERPRETATION GUIDE ===\n")
cat("*** = Best model (lowest AIC/BIC)\n")
cat("*   = Competitive model (within 2 points)\n")
cat("Delta < 2: Models essentially equivalent\n")
cat("Delta 2-7: Some support for better model\n") 
cat("Delta >7: Strong support for better model\n")
cat("Conv. = Model converged successfully\n")
```

# Detailed Results for Each Outcome

```{r extract-results-function}
# Function to extract and display key results for best models
extract_key_results <- function(outcome_name) {
  
  models <- model_results[[outcome_name]]
  if(is.null(models)) {
    cat("No models available for", outcome_name, "\n")
    return(NULL)
  }
  
  cat("\n=== RESULTS FOR", toupper(outcome_name), "===\n")
  
  # Block model results (Age × Block)
  if(!is.null(models$block_model)) {
    cat("\nAge × Block Model:\n")
    block_summary <- summary(models$block_model)
    print(round(block_summary$coefficients, 4))
  }
  
  # Target model results (compare fixed vs random)
  best_target_model <- model_comparison %>%
    filter(Outcome == outcome_name, 
           Model %in% c("Age_x_Target_Fixed", "Age_Target_Random")) %>%
    slice_min(AIC)
  
  if(nrow(best_target_model) > 0) {
    if(best_target_model$Model[1] == "Age_x_Target_Fixed") {
      cat("\nBest Target Model: Age × Target (Fixed Effects)\n")
      if(!is.null(models$target_fixed)) {
        target_summary <- summary(models$target_fixed)
        print(round(target_summary$coefficients, 4))
      }
    } else {
      cat("\nBest Target Model: Age + Target (Random Effects)\n")
      if(!is.null(models$target_random)) {
        target_summary <- summary(models$target_random)
        print(round(target_summary$coefficients, 4))
        var_components <- as.data.frame(VarCorr(models$target_random))
        target_var <- var_components$vcov[var_components$grp == "Target"]
        if(length(target_var) > 0) {
          cat("Target Random Effect Variance:", round(target_var[1], 4), "\n")
        }
      }
    }
  }
}
```

```{r detailed-results}
# Show detailed results for each outcome
for(outcome in outcomes) {
  extract_key_results(outcome)
}
```

# Summary and Recommendations

## Overfitting Assessment Summary

```{r overfitting-summary-table}
# Display overfitting summary
if(nrow(overfitting_summary) > 0) {
  overfitting_summary %>%
    mutate(
      Block_Status = ifelse(Block_Model_OK, "✅ OK", "⚠️ Issues"),
      Target_Fixed_Status = ifelse(Target_Fixed_OK, "✅ OK", "⚠️ Issues"),
      Target_Random_Status = ifelse(Target_Random_OK, "✅ OK", "⚠️ Issues")
    ) %>%
    select(Outcome, Block_Status, Target_Fixed_Status, Target_Random_Status) %>%
    kable(caption = "Overfitting Assessment Summary",
          col.names = c("Outcome", "Block Model", "Target Fixed Model", "Target Random Model")) %>%
    kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
} else {
  cat("No overfitting assessment data available.\n")
}
```

## Convergence Issues Summary

```{r convergence-summary}
# Create convergence summary table
convergence_summary <- data.frame()

for(outcome in names(convergence_issues)) {
  conv_info <- convergence_issues[[outcome]]
  
  temp_summary <- data.frame(
    Outcome = outcome,
    Block_Converged = ifelse(is.null(conv_info$error), conv_info$block_converged, FALSE),
    Target_Fixed_Converged = ifelse(is.null(conv_info$error), conv_info$target_fixed_converged, FALSE),
    Target_Random_Converged = ifelse(is.null(conv_info$error), conv_info$target_random_converged, FALSE),
    Error_Message = ifelse(is.null(conv_info$error), "None", conv_info$error)
  )
  
  convergence_summary <- rbind(convergence_summary, temp_summary)
}

# Display convergence issues
if(nrow(convergence_summary) > 0) {
  convergence_summary %>%
    mutate(
      Block_Status = ifelse(Block_Converged, "✅", "❌"),
      Target_Fixed_Status = ifelse(Target_Fixed_Converged, "✅", "❌"),
      Target_Random_Status = ifelse(Target_Random_Converged, "✅", "❌")
    ) %>%
    select(Outcome, Block_Status, Target_Fixed_Status, Target_Random_Status, Error_Message) %>%
    kable(caption = "Model Convergence Summary",
          col.names = c("Outcome", "Block Model", "Target Fixed", "Target Random", "Error")) %>%
    kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
}
```

## Prevention Strategies

**If you detect overfitting or convergence issues:**

1. **Simplify random effects structure**:
   ```r
   # Instead of: (Block | Participant) 
   # Use: (1 | Participant)
   
   # Instead of: (1 | Target) + (1 | Participant)
   # Use just: (1 | Participant)
   ```

2. **Reduce fixed effects complexity**:
   ```r
   # Remove three-way interactions
   # Use main effects instead of interactions if justified
   ```

3. **Consider regularization** (for complex models):
   ```r
   # Use glmmTMB with penalties or Bayesian approaches
   ```

4. **Check for outliers and data quality issues**:
   ```r
   # Remove extreme outliers that might cause convergence problems
   # Check for missing data patterns
   ```

## Cross-Validation Check

```{r cross-validation-setup, eval=FALSE}
# Example cross-validation for overfitting detection
library(caret)

# Function for k-fold CV on mixed models
cv_mixed_model <- function(data, formula_str, k = 5) {
  
  # Create folds by participant (to respect clustering)
  participants <- unique(data$Participant)
  folds <- createFolds(participants, k = k, returnTrain = TRUE)
  
  cv_results <- list()
  
  for(i in 1:k) {
    # Split by participants
    train_participants <- participants[folds[[i]]]
    train_data <- data[data$Participant %in% train_participants, ]
    test_data <- data[!data$Participant %in% train_participants, ]
    
    # Fit model on training data
    tryCatch({
      model <- lmer(as.formula(formula_str), data = train_data)
      
      # Predict on test data
      predictions <- predict(model, newdata = test_data, allow.new.levels = TRUE)
      actual <- test_data[[all.vars(as.formula(formula_str))[1]]]
      
      # Calculate RMSE
      rmse <- sqrt(mean((predictions - actual)^2, na.rm = TRUE))
      cv_results[[i]] <- rmse
    }, error = function(e) {
      cv_results[[i]] <- NA
    })
  }
  
  return(list(
    mean_rmse = mean(unlist(cv_results), na.rm = TRUE),
    sd_rmse = sd(unlist(cv_results), na.rm = TRUE),
    individual_rmse = unlist(cv_results),
    n_successful_folds = sum(!is.na(unlist(cv_results)))
  ))
}

# Example usage (uncomment to run):
# cv_result <- cv_mixed_model(nav_data, "Navigation_Time ~ Age * Block_Num + (1|Participant)")
# print(cv_result)
```

## Model Selection Summary

```{r model-summary}
# Summarize model preferences across outcomes (only for converged models)
summary_table <- model_comparison %>%
  filter(Converged == TRUE) %>%  # Only include converged models
  group_by(Outcome) %>%
  slice_min(AIC) %>%
  ungroup() %>%
  count(Model, name = "N_outcomes_best") %>%
  mutate(Percentage = round(N_outcomes_best / length(outcomes) * 100, 1))

summary_table %>%
  kable(caption = "Model Preferences Across All Outcomes (Based on AIC, Converged Models Only)",
        col.names = c("Model", "# Outcomes Best", "% of Outcomes")) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

## Recommendations

Based on the model comparison results:

1. **For Age × Block analyses**: Use the Age × Block model for your main research questions about learning differences between age groups.

2. **For Target analyses**: 
   ```{r target-recommendation}
   # Count wins for target models among converged models only
   target_model_comparison <- model_comparison %>%
     filter(Model %in% c("Age_x_Target_Fixed", "Age_Target_Random"), 
            Converged == TRUE) %>%
     group_by(Outcome) %>%
     slice_min(AIC) %>%
     ungroup()
   
   target_random_wins <- sum(target_model_comparison$Model == "Age_Target_Random")
   total_target_comparisons <- nrow(target_model_comparison)
   
   if(target_random_wins > total_target_comparisons/2) {
     cat("- Consider Target as RANDOM effect - it wins for", target_random_wins, "out of", total_target_comparisons, "converged outcomes\n")
     cat("- This suggests meaningful target-to-target variability\n")
     cat("- Better for generalizability to other navigation targets\n")
   } else {
     cat("- Consider Target as FIXED effect - random effect doesn't consistently improve model fit\n")
     cat("- This suggests your specific targets are what matter\n")
     cat("- Better for making specific claims about particular targets\n")
   }
   ```

3. **Multiple comparisons**: Based on your analysis plan, consider grouping related outcomes for family-wise correction.

4. **Regarding your specific questions**:
   - **Single comprehensive model vs. separate models**: Given the complexity and potential overfitting concerns, stick with your proposed separate models approach. This is more interpretable and less prone to convergence issues.
   - **Multiple comparison correction across outcomes**: Your reasoning against cross-outcome correction is sound. The outcomes measure conceptually distinct aspects of navigation behavior. Consider reporting both uncorrected and FDR-corrected results as supplementary information.

## Data Quality Checks

```{r data-quality}
# Quick data quality assessment
cat("=== DATA QUALITY ASSESSMENT ===\n")

for(outcome in outcomes) {
  if(outcome %in% names(nav_data)) {
    outcome_data <- nav_data[[outcome]]
    
    cat("\n", outcome, ":\n")
    cat("  Missing values:", sum(is.na(outcome_data)), "\n")
    cat("  Range: [", round(min(outcome_data, na.rm = TRUE), 2), ", ", 
        round(max(outcome_data, na.rm = TRUE), 2), "]\n", sep = "")
    
    # Check for extreme outliers (beyond 3 SD)
    if(is.numeric(outcome_data)) {
      outliers <- abs(scale(outcome_data)) > 3
      cat("  Extreme outliers (>3 SD):", sum(outliers, na.rm = TRUE), "\n")
    }
  } else {
    cat("\n", outcome, ": COLUMN NOT FOUND IN DATA\n")
  }
}
```

## Session Info

```{r session-info}
sessionInfo()
```